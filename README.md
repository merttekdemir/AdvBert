#Intorduction
The modern world is filled with an incredible abundance of text data, however much of this data is unlabled for any practical text classification purpose. With the advent of transformers and the subsequent BERT model by Devlin et al. in 2018 large strides were taken towards making this wide repository of unlabeled data more accessible for machine learning tasks.
The BERT model comes in two sizes, "base" and "large", and the models consist of 110 million parameters and 340 million parameters respectively and have been trained on massive corpora. These massive models therefore admit a strong pretrained general language understanding that allow them to be fine-tuned to downstream tasks very successfully with relatively little labeled data. Typically for class sizes less than 10, datasets on the order of 10.000 labeled examples are sufficient, possibly even on the order of 1.000 depending on the task.
Due to the cost of curating labeled text datasets and the performance of the BERT model compared to its predecessors this represents a monumental step forward in democratizing text classification tasks. Nevertheless, for niche tasks, prototyping or small research/enterprise endevors labeled datasets on the order of thousands or tens of thousands may still be prohibitively expensive.
One recent proposed solution to reduce the labeled example burden further has been the use of an adversarial and semi-supervised fine tuning procedure through GAN networks, Croce et al in 2020. This method allows to augment the existing labelled dataset both with unlabelled data as well as "fake" generated data from the Generator network during triaining. It was shown by the authors to significantly outperform the Bert model on fintuning tasks where only a handful of labeled examples are provided, typically less than 1000. However, the performance of this model quickly converges to that of the typical Bert model as the percentage of annotated examples present in the dataset grows, typically converging in performance as the annotated examples approach 10-20% of the dataset.
One alternative adversarial and semi-supervised solution that may potentially achieve a similar affect is that of adversarial training and virtual adversarial training for text classificaiton, proposed originally for LSTMs by Miyato et al in 2017. Although originally intended to regularize loss functions and not for low-data schemes, this technique also augments the existing dataset to adversarially fine tune a model and can work both in the supervised and unsupervised setting. It thus exhibits obvious parallels to the semi-supervised GAN technique. Further, although it achieved state of the art performance on its original release, to my knowledge the technique has not been applied to transformer based models yet and remains largely unexplored. One reason for this may be the subsequent publication of the BERT model which rendered LSTM's largely outdated and spurred a new line of research in the NLP field.
Beyond the parallels to the proven SS-GAN technique, adversarial/generative adversarial learning may provide strong synergies with the SS-GAN technique as well. While the GAN method is applied on the classification, and therefore output side of a model, the adversarial/generative adversarial technique is applied on the embedding, and therefore input side of a model. Moreover, the two adversarial techniques can also be applied symultaneously. Thus the three techniques may combine for an end to end adversarial and semi-supervised text classification technique performant on low data environments.
Finally, although reasons for the convergence between the vanilla BERT and GAN-BERT models were not discussed by the original authors, the adversarial techniques act as regularizers shown to prevent overfitting and increase test accuracies on larger datasets. As such there is reason to believe a model leveraging the full suite of techniques should continue to outperform the vanilla Bert model regardless of the percent of annotated data in the dataset.
What follows in the notebook is a high level introduction to the various moving parts necessary to understand this model as well as code to run an experiment of this model on the IMDB dataset.
